{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extraordinary-rwanda",
   "metadata": {},
   "source": [
    "# **Behavior Analysis using DeepLabCut:**\n",
    "This notebook will illustrate how to:\n",
    "\n",
    "* Create a new DeepLabCut project\n",
    "* Sample videos to obtain frames\n",
    "* Label the bodyparts in the frames\n",
    "* Creating a training dataset\n",
    "* Training a network\n",
    "* Evaluating the network\n",
    "* Analyzing videos\n",
    "\n",
    "\n",
    "Nath, Mathis et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019. Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
    "\n",
    "**All the necessary toolkits are already present in the LINdoscope environment. Please follow the instructions for each cell and enjoy !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaning-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-objective",
   "metadata": {},
   "source": [
    "## 1. Create a new project :\n",
    "You can use this function to create a new project with sub-directories and a basic configuration file in the user defined directory. Otherwise the project is created in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "democratic-illustration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\videos\"\n",
      "Created \"C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\labeled-data\"\n",
      "Created \"C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\training-datasets\"\n",
      "Created \"C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\dlc-models\"\n",
      "Copying the videos\n",
      "C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\videos\\vame_freelymoving_mouse_bottomcam.mp4\n",
      "Generated \"C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\config.yaml\"\n",
      "\n",
      "A new project with name lindoscope_analysis-LINdoscope-2021-08-30 is created at C:\\Users\\compraka\\Research\\LINdoscope\\analysis and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "task='lindoscope_analysis' # Enter the name of your experiment Task\n",
    "experimenter='LINdoscope' # Enter the name of the experimenter\n",
    "working_directory= r'C:\\Users\\compraka\\Research\\LINdoscope\\analysis' # Enter the path to the working directory where you want to\n",
    "                                                                     # create the project\n",
    "videos=[r'C:\\Users\\compraka\\Research\\LINdoscope\\videos\\vame_freelymoving_mouse_bottomcam.mp4'] # Enter the paths of your videos \n",
    "                                                                                               # where you want to grab frames from\n",
    "\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,videos,working_directory,copy_videos=True, videotype=\".mp4\", multianimal=False) \n",
    "# The function returns the path, where your project is\n",
    "# Check the folder to see if the config file has been created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-bottle",
   "metadata": {},
   "source": [
    "## Edit the config.yaml files\n",
    "**The parameters that you can adjust are:**\n",
    "\n",
    "* Number of frames to extract for a video (change the 'numframes2pick' parameter)\n",
    "* Labels for the bodypart markers (list the bodyparts you would like to label. eg. tailtip, eartipleft etc.)\n",
    "* Size of the markers (change the 'dotsize' parameter)\n",
    "* p cutoff value (you want this value to be high, this ensures the network plots the points it is confident about)\n",
    "* Croping of the video (you can also crop the video by setting the 'crop' parameter to 'True' and then adjusting the x1,x2,y1 and y2 parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-check",
   "metadata": {},
   "source": [
    "## 2. Extract frames from the video :\n",
    "You can use this function to extract frames from the videos to label the bodyparts. A succesfulfeature detector requires a diverse selection of frames. The frames can be extracted using uniform sampling, k-means sampling or manual selection. This is a good point to also check how the cropped frames look, if you have chosen to crop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-enclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\compraka\\Research\\LINdoscope\\analysis\\lindoscope_analysis-LINdoscope-2021-08-30\\videos\\vame_freelymoving_mouse_bottomcam.mp4 ?\n",
      "yes/noyes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 1784.44  seconds.\n",
      "Extracting and downsampling... 44611  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44611it [09:39, 76.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\compraka\\Anaconda3\\envs\\DLC-GPU\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:888: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 10240 or by setting the environment variable OMP_NUM_THREADS=1\n",
      "  f\"MiniBatchKMeans is known to have a memory leak on \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames were successfully extracted, for the videos of interest.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (if you extracted enough frames for all videos).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "extracted_frame = deeplabcut.extract_frames(path_config_file,  mode=\"automatic\", algo=\"kmeans\") # Automatic k-means frame\n",
    "# extraction. Set algo='uniform' to extract the frames uniformly\n",
    "extracted_frame = deeplabcut.extract_frames(path_config_file,  mode=\"manual\") # Manual extraction of frames\n",
    "plt.imshow(extracted_frame) # Plot the frame to make sure the cropping parameters work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-playlist",
   "metadata": {},
   "source": [
    "## 3. Labelling the frames :\n",
    "Now you can label the extracted frames with the bodypart labels you defined in the config file. The labeled images will be saved in the project directory in the sub-directory 'labeled-data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "\n",
    "deeplabcut.label_frames(path_config_file)\n",
    "# At this point you can check the labels since this is the most cucial step for creating a training dataset.\n",
    "\n",
    "deeplabcut.check_labels(path-config_file)\n",
    "# You can now check the frames with your labels in the 'check-labels' subdirectory\n",
    "# You can also adjust the labels by relaunching the labeling GUI and move the labels around and click save."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-workplace",
   "metadata": {},
   "source": [
    "## 4. Creating a training dataset :\n",
    "You can create a training dataset using the function provided below. This geenrates the training information that the network will use. You can adjust the training and test fraction in the config.yaml file by changing the 'TrainingFraction' parameter. You can also create multiple shuffles of the data to benchmark performance, the default is set to 1. There are also several networks that you can pick from, the default network used here is the resnet-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-australia",
   "metadata": {},
   "source": [
    "## 5. Start training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-auckland",
   "metadata": {},
   "source": [
    "## 6. Evaluate the network :\n",
    "After the network has trained, you need to evaluate how your network has learnt. To do this, you can use the following function which will store the results as a .csv file and also plots the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluae_network(path_config_file, plotting=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-context",
   "metadata": {},
   "source": [
    "## 7. Analyze your videos :\n",
    "If you ae happy with the training of your network, you can now start analyzing your videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the videos you want to analyze:\n",
    "\n",
    "analyze_videopath = ['video/video1.mp4', 'video/video2.mp4']\n",
    "deeplabcut.analyze_videos(path_config_file, analyze_videopath, videotype='.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-december",
   "metadata": {},
   "source": [
    "## 7a. Extract outlier frames :\n",
    "\n",
    "When the evaluation results are not satisfactory, and the labels are predicted poorly, then you can extract frames in which the labels are predicted incorrectly and refine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.extact_outlier_frames(path_config_file, ['video/video1.mp4']) # You need to provide a specific video here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file) # This opens the GUI and allows you to adjust the markers in the newly \n",
    "                                           # extacted frames\n",
    "deeplabcut.merge_datasets(pah_config_file) # This merges the new labels with the older labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the new labels are merged with the original dataset, there is a new iteration of training datasets created \n",
    "#automatically and adjusted in the config.yaml file. You can now reate a training dataset and start training\n",
    "deeplabcut.create_training_dataset(path_config_file)\n",
    "deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-hardwood",
   "metadata": {},
   "source": [
    "## 8. Creating labeled videos :\n",
    "Now you can visualize the fruits of your labour, the following function can be used to create .mp4 videos with the labels predicted by the network. The videos are saved in the same directory as the original videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file)\n",
    "\n",
    "# Plot the trajectories of the analyzed vidoes\n",
    "%matplotlib notebook\n",
    "deeplabcut.plot_trajectories(path_config_file, analyz_videopath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

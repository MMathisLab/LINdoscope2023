{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Welcome to the VAME behaviour analysis lab\n",
    " \n",
    "In this notebook we want to teach you a state-of-the-art deep learning method for behavior quantification called\n",
    "Variational Animal Motion Embedding ([VAME](https://github.com/LINCellularNeuroscience/VAME)). This method is based on artificial recurrent neural networks and makes use of the powerful [variational autoencoder](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) framework.\n",
    "\n",
    "## Part 0: Setting up the environment  and data alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import of necessary functions\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Research\\\\vame_alpha\\\\VAME\\\\\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from display_video import play_motif_video\n",
    "\n",
    "import vame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project name\n",
    "project = \"Lindoscope-VAME\"\n",
    "\n",
    "# set working directory\n",
    "working_directory = \"C:\\\\Research\\\\Lindoscope\\\\\"\n",
    "\n",
    "# specifying path to example video\n",
    "videos = ['C:\\\\Research\\\\Lindoscope\\\\video-1.mp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize your project\n",
    "config = vame.init_new_project(project=project, videos=videos, working_directory=working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize config\n",
    "config = \"C:\\\\Research\\\\Lindoscope\\\\Lindoscope-VAME-Aug18-2021\\\\config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialization of the project, please go to your folder and move video-1.csv into \\videos\\pose_estimation\\ in your VAME project folder.\n",
    "\n",
    "#### Egocentrical alignment\n",
    "Currently, the recorded open-field data is in an allocentric representation. This means, our DeepLabCut x-y-coordinates are representing the location within the arena instead of the kinematis of the mouse movements. Therefore, we align our dataset egocentrical in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Egocentric alignment of the input data\n",
    "# pose_ref_index: list of reference coordinate indices for alignment\n",
    "# Example: 0: snout, 1: forehand_left, 2: forehand_right, 3: hindleft, 4: hindright, 5: tail\n",
    "vame.egocentric_alignment(config, pose_ref_index=[0,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the alignment, we can inspect it via plotting an example trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path_to_trajectory = \"C:\\\\Research\\\\Lindoscope\\\\Lindoscope-VAME-Aug18-2021\\\\data\\\\video-1\\\\video-1-PE-seq.npy\"\n",
    "trajectory = np.load(path_to_trajectory)\n",
    "\n",
    "# set plotting configuration\n",
    "points = 3000\n",
    "start = np.random.choice(trajectory.shape[1] - points)\n",
    "end = start + points\n",
    "window=slice(start,end)\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(trajectory[:,window].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training a VAME model\n",
    "This part of the notebook creates a training set from the egocentric aligned time series and initializes a VAME model to learn a lower dimensional embedding of the input data, which hopefully captures spatiotemporal characteristics of the input data to then identify behavioral motifs. \n",
    "\n",
    "First, we will create a training dataset which only takes a few moments. Then, we initialize a VAME model and let this model train for a few epochs. If you are using your own dataset, please talk to your TA about how long you should train the model. If you using the example dataset, train the model for about 30 epochs (~5 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set for the VAME model\n",
    "vame.create_trainset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vame.train_model(config, softbool=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained our model, its time to evuluate its performance. As VAME is an unsupervised model, we don't have a ground truth to check its accuracy. What we can check, however, is its capability to reconstruct input signals. In a later step, we sample from its embedding distribution to further investigate the qualitiy of the learned representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "vame.evaluate_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see that the red dotted lines in the reconstruction panel follow the black lines, we can assume that the model learned a proper embedding space. The next part of this notebook is concerend with inference and obtaining behavioral motifs.\n",
    "\n",
    "## Part 2: Inference of behavioral motifs\n",
    "To infer behavioral motifs, the trained model runs on the entire dataset and embeds spatiotemporal characteristics into a lower dimensional representation or latent vectors. Using the latent vector information, a k-Means clustering then infers a cluster assignment which represents the motifs. \n",
    "\n",
    "Note, that the newest VAME version uses a Hidden-Markov-Model (HMM) to infer hidden states from the latent vector information. While this is more robust and leads to a better quantification, it can, however, take long to infer these hidden states depending on the size of your dataset. Therefore, we use here a k-Means assignment but your are more then welcome to try later the HMM as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment motifs\n",
    "vame.pose_segmentation(config, parameterization=\"kmeans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check our embedding space by a) reconstructing some real samples from this space and b) by sampling new, unseen samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the generative model (reconstruction decoder) to sample from \n",
    "# the learned data distribution or reconstruct random real samples \n",
    "#options: mode: \"reconstruction\", \"sampling\"\n",
    "vame.generative_model(config, mode=\"sampling\", parameterization=\"kmeans\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we run the inference and checked the embedded latent space we can finally investigate how our inferred motifs look by creating a video representation. For every behavioral motif, a video will be created. This takes a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vame.motif_videos(config, param='kmeans', videoType='.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created our motif videos we can display them in the next function. Note that you can quit a video by pressing \"q\".\n",
    "If you have trouble with the video, you can also just go manually to the folder:\n",
    "\"your-vame-project\\\\resulst\\\\file\\\\model\\\\parameterization-num\\\\cluster_videos\\\\...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function will open a window which contains the video\n",
    "play_motif_video(config, motif=5, parameterization=\"kmeans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analysis\n",
    "The next part will teach you on how to analys the motif data and to identify structure in the data. Especially, we will look at the motif distribution, create communities (groups of motifs) and also investigate their transition probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distribution import behavior_distribution\n",
    "from transitions import markov_graph_analysis, community_ordered_transition_probabilities\n",
    "from tree import create_tree_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the distribution of behavioral motifs\n",
    "behavior_distribution(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_graph_analysis(config, param='kmeans', threshold=0.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tree_representation(config, param='kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree or hierarchical representation can help you to group motifs into communities or categories. For the next part of the analysis, you should group the tree. This needs to be done by hand. Therefore, please fill out the list below with the tree labels from left to right. Example:\n",
    "\n",
    "                                    root\n",
    "                                  /      \\\n",
    "                                 o        o\n",
    "                                / \\      / \\ \n",
    "                               4   o    1   3\n",
    "                                  / \\\n",
    "                                 5   2\n",
    "                              \n",
    "community_order = [4,5,2,1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_order = [13,14,7,0,12,8,4,10,1,11,6,5,3,9,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ordered_transition_probabilities(config, community_order, param='kmeans', threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
